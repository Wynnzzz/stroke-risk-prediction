# -*- coding: utf-8 -*-
"""stroke_risk_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15AmthISrq_Iw5J5t3XhGci4iRvoklNa7

# Import Packages
"""

import warnings
warnings.filterwarnings('ignore')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report

"""# Loading Data"""

df=pd.read_csv('/kaggle/input/stroke-risk-prediction-dataset/stroke_risk_dataset.csv')
df.head()

"""Pada tahap awal, dataset dibaca menggunakan fungsi pd.read_csv() dari library Pandas, yang bertujuan untuk memuat file CSV bernama stroke_risk_dataset.csv dari direktori yang telah ditentukan. Dataset yang berhasil dimuat kemudian disimpan dalam sebuah objek DataFrame bernama df. Setelah itu, fungsi df.head() digunakan untuk menampilkan lima baris pertama dari dataset, guna memberikan gambaran awal mengenai struktur data, seperti nama kolom, jenis data, serta nilai-nilai awal pada setiap fitur. Tahapan ini penting sebagai langkah awal eksplorasi data sebelum dilakukan proses pembersihan, transformasi, dan analisis lebih lanjut."""

df.info()

"""Perintah df.info() digunakan untuk melihat ringkasan struktur dataset, seperti jumlah entri, jumlah kolom, nama kolom, tipe data, dan jumlah nilai non-null."""

features=df.columns
features

"""features = df.columns menyimpan daftar nama semua kolom (fitur) dalam variabel features.

# Exploratory Data Analysis (EDA)
"""

df.describe()

"""df.describe() memberikan statistik deskriptif seperti mean, std, dan quartiles untuk kolom numerik."""

df.isnull().sum()

"""df.isnull().sum() digunakan untuk mengecek jumlah nilai yang hilang (missing) di setiap kolom. Pada tahapan ini tidak ditemukan data hilang."""

df.duplicated().sum()

"""df.duplicated().sum() menghitung jumlah baris duplikat. Pada perintah ini ditemukan terdapat 1021 data yang terduplikasi"""

df.drop_duplicates(inplace=True)

"""df.drop_duplicates(inplace=True) digunakan untuk menghapus baris duplikat tersebut dari dataset secara permanen."""

n_cols=3
n_rows=-(-(len(df.columns)//n_cols))+1

fig,axes=plt.subplots(n_rows, n_cols, figsize=(20,3*n_rows))

axes=axes.flatten()

for idx, feature in enumerate(features):
    sns.boxplot(x=df[feature], ax=axes[idx])
    axes[idx].set_title(f'Box Plot of {feature}')

for i in range(len(features), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

"""Kode ini berfungsi untuk membuat visualisasi boxplot dari setiap fitur (kolom) di dataset secara rapi dalam beberapa baris dan kolom pada satu figure besar. Ini membantu melihat distribusi dan deteksi outlier tiap fitur secara visual."""

n_cols=3
n_rows=-(-(len(df.columns)//n_cols))+1

fig,axes=plt.subplots(n_rows, n_cols, figsize=(20,3*n_rows))

axes=axes.flatten()

for idx, feature in enumerate(features):
    sns.histplot(data=df, x=feature, kde=True, ax=axes[idx], color='blue')
    axes[idx].set_title(f'Distribution of {feature}')

for i in range(len(features), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

"""Kode ini membuat visualisasi distribusi tiap fitur dalam dataset menggunakan histogram dengan kurva KDE (Kernel Density Estimate) secara rapi dalam grid subplot. Visualisasi ini membantu memahami bentuk distribusi data setiap kolom, seperti simetri, skewness, dan keberadaan multimodalitas."""

correlation_matrix=df[features].corr()

plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""Kode tersebut membuat visualisasi heatmap korelasi antar fitur dalam dataset. Heatmap ini berguna untuk melihat hubungan linear antar fitur, misalnya fitur mana yang berkorelasi kuat positif atau negatif, sehingga bisa membantu memilih fitur yang penting atau menghindari multikolinearitas.

# Building Models
"""

X=df.drop(columns=['Stroke Risk (%)', 'At Risk (Binary)'])
y=df['At Risk (Binary)']

"""Kode ini melakukan pemisahan dataset menjadi fitur dan target untuk keperluan modeling."""

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

"""Kode ini melakukan normalisasi fitur dengan menggunakan StandardScaler dari scikit-learn. Tujuannya supaya fitur-fitur pada dataset memiliki skala yang sama agar model machine learning bisa belajar lebih efektif dan hasilnya lebih stabil."""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f'Training set shape: X_train={X_train.shape}, y_train={y_train.shape}')
print(f'Test set shape: X_test={X_test.shape}, y_test={y_test.shape}')

"""Kode tersebut melakukan pembagian data menjadi data latih dan data uji menggunakan fungsi train_test_split dari scikit-learn. Ini merupakan langkah penting agar model dapat dilatih dan dievaluasi secara adil.

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

# Hyperparameter tuning
param_grid_lr = {
    'C': [0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear']
}

grid_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')
grid_lr.fit(X_train, y_train)

print("Best Logistic Regression:", grid_lr.best_params_)
y_pred_lr = grid_lr.predict(X_test)
print(classification_report(y_test, y_pred_lr))

"""Kode ini melakukan pencarian hyperparameter terbaik untuk model Logistic Regression menggunakan GridSearchCV dengan validasi silang 5-fold, kemudian melatih model tersebut pada data latih, memprediksi data uji, dan menampilkan laporan klasifikasi yang berisi metrik evaluasi seperti precision, recall, dan f1-score untuk menilai performa model dalam memprediksi risiko stroke.

## Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')
grid_rf.fit(X_train, y_train)

print("Best Random Forest:", grid_rf.best_params_)
y_pred_rf = grid_rf.predict(X_test)
print(classification_report(y_test, y_pred_rf))

"""Kode ini melakukan pencarian hyperparameter terbaik untuk model Random Forest menggunakan GridSearchCV dengan validasi silang 5-fold. Parameter yang diuji meliputi jumlah pohon keputusan (n_estimators), kedalaman maksimum pohon (max_depth), jumlah minimum sampel untuk membagi node (min_samples_split), dan jumlah minimum sampel di daun (min_samples_leaf). Setelah menemukan kombinasi terbaik, model dilatih pada data latih, kemudian digunakan untuk memprediksi data uji, dan hasilnya dievaluasi dengan laporan klasifikasi yang menampilkan metrik seperti precision, recall, dan f1-score untuk menilai performa model dalam memprediksi risiko stroke."""

from xgboost import XGBClassifier

param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
}

grid_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
                        param_grid_xgb, cv=5, scoring='accuracy')
grid_xgb.fit(X_train, y_train)

print("Best XGBoost:", grid_xgb.best_params_)
y_pred_xgb = grid_xgb.predict(X_test)
print(classification_report(y_test, y_pred_xgb))

"""Kode ini melakukan tuning hyperparameter untuk model XGBoost Classifier menggunakan GridSearchCV dengan validasi silang 5-fold. Parameter yang diuji adalah jumlah estimator (n_estimators), kedalaman maksimum pohon (max_depth), learning rate, dan subsample. Setelah menemukan kombinasi parameter terbaik, model dilatih dengan data latih, kemudian memprediksi data uji, dan performanya dievaluasi menggunakan classification report yang mencakup metrik seperti precision, recall, dan f1-score untuk mengukur akurasi prediksi risiko stroke."""